{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392bb383-72f0-4082-802f-bc408795670e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:39:41.801256Z",
     "iopub.status.busy": "2024-01-25T01:39:41.800722Z",
     "iopub.status.idle": "2024-01-25T01:39:47.768146Z",
     "shell.execute_reply": "2024-01-25T01:39:47.766766Z",
     "shell.execute_reply.started": "2024-01-25T01:39:41.801209Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 01:39:44.335191: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-25 01:39:44.397954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38214686-0065-40e7-962c-5edadbda9731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:39:47.772405Z",
     "iopub.status.busy": "2024-01-25T01:39:47.771472Z",
     "iopub.status.idle": "2024-01-25T01:40:28.206425Z",
     "shell.execute_reply": "2024-01-25T01:40:28.205499Z",
     "shell.execute_reply.started": "2024-01-25T01:39:47.772351Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]:  [[[118  84  73]\n",
      "  [134  94  86]\n",
      "  [167 117 114]\n",
      "  ...\n",
      "  [205 158 126]\n",
      "  [243 223 222]\n",
      "  [255 253 255]]\n",
      "\n",
      " [[128  91  84]\n",
      "  [141  99  94]\n",
      "  [169 115 118]\n",
      "  ...\n",
      "  [205 158 120]\n",
      "  [243 221 213]\n",
      "  [255 250 249]]\n",
      "\n",
      " [[149 107 106]\n",
      "  [157 108 113]\n",
      "  [172 112 129]\n",
      "  ...\n",
      "  [206 158 107]\n",
      "  [244 217 192]\n",
      "  [255 244 229]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[140 193 197]\n",
      "  [128 177 181]\n",
      "  [101 142 145]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[111 169 164]\n",
      "  [102 154 147]\n",
      "  [ 82 123 110]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[ 98 158 148]\n",
      "  [ 90 144 131]\n",
      "  [ 73 114  94]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "labels[0: ] 0\n"
     ]
    }
   ],
   "source": [
    "imgs_path = \"./Train\"\n",
    "data = []\n",
    "labels = []\n",
    "CLASSES = 43\n",
    "# using for loop to access each image\n",
    "for i in range(CLASSES):\n",
    "    img_path = os.path.join(imgs_path, str(i)) #0-42\n",
    "    for img in os.listdir(img_path):\n",
    "        im = Image.open(imgs_path + '/' + str(i) + '/' + img)\n",
    "        im = im.resize((75,75))\n",
    "        im = np.array(im)\n",
    "        data.append(im)\n",
    "        labels.append(i)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(\"data[0]: \",data[0])\n",
    "print(\"labels[0: ]\",labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2fbe31-f672-47c8-8bd7-43ceb7c085db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:28.208028Z",
     "iopub.status.busy": "2024-01-25T01:40:28.207535Z",
     "iopub.status.idle": "2024-01-25T01:40:28.441537Z",
     "shell.execute_reply": "2024-01-25T01:40:28.440443Z",
     "shell.execute_reply.started": "2024-01-25T01:40:28.207986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape:  (31367, 75, 75, 3) (31367,)\n",
      "testing shape:  (7842, 75, 75, 3) (7842,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(\"training shape: \",x_train.shape, y_train.shape)\n",
    "print(\"testing shape: \",x_val.shape, y_val.shape)\n",
    "# convert interge label to one-hot data\n",
    "y_train = to_categorical(y_train, 43)\n",
    "y_val = to_categorical(y_val, 43)\n",
    "\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57882212-c25c-40bc-8d7a-9db480620aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:28.448125Z",
     "iopub.status.busy": "2024-01-25T01:40:28.445627Z",
     "iopub.status.idle": "2024-01-25T01:40:35.155807Z",
     "shell.execute_reply": "2024-01-25T01:40:35.154665Z",
     "shell.execute_reply.started": "2024-01-25T01:40:28.448075Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test = pd.read_csv(\"./Test.csv\")\n",
    "test_labels = test['ClassId'].values.tolist()\n",
    "\n",
    "test_img_path = \"./\"\n",
    "test_imgs = test['Path'].values\n",
    "test_data = []\n",
    "\n",
    "for img in test_imgs:\n",
    "    im = Image.open(test_img_path + '/' + img)\n",
    "    im = im.resize((75,75))\n",
    "    im = np.array(im)\n",
    "    test_data.append(im)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e04a5d7-68d8-45d2-b2ed-368f3eb8b5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:35.158973Z",
     "iopub.status.busy": "2024-01-25T01:40:35.157116Z",
     "iopub.status.idle": "2024-01-25T01:40:35.163435Z",
     "shell.execute_reply": "2024-01-25T01:40:35.162633Z",
     "shell.execute_reply.started": "2024-01-25T01:40:35.158932Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486960f-ef58-4b34-a82b-a1938c167f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f9ec9-714b-42fa-9462-cc979722153d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9da4649b-45ac-4500-a725-3af9dabe88ce",
   "metadata": {},
   "source": [
    "# Transfer Learning using Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eed8abd-6a63-4687-b9ad-ca51ced45578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:35.166018Z",
     "iopub.status.busy": "2024-01-25T01:40:35.165327Z",
     "iopub.status.idle": "2024-01-25T01:40:37.263869Z",
     "shell.execute_reply": "2024-01-25T01:40:37.262564Z",
     "shell.execute_reply.started": "2024-01-25T01:40:35.165983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "\n",
    "xception = Xception(weights= 'imagenet', include_top=False, input_shape= (75,75,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1fa215-371f-4f63-9b1a-9be241955e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:37.267289Z",
     "iopub.status.busy": "2024-01-25T01:40:37.266938Z",
     "iopub.status.idle": "2024-01-25T01:40:37.310711Z",
     "shell.execute_reply": "2024-01-25T01:40:37.309829Z",
     "shell.execute_reply.started": "2024-01-25T01:40:37.267244Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "x = xception.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(43, activation= 'softmax')(x)\n",
    "model = Model(inputs = xception.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0dda9ef-22ea-4e36-9c51-335b441865ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:37.315014Z",
     "iopub.status.busy": "2024-01-25T01:40:37.314716Z",
     "iopub.status.idle": "2024-01-25T01:40:37.340746Z",
     "shell.execute_reply": "2024-01-25T01:40:37.339666Z",
     "shell.execute_reply.started": "2024-01-25T01:40:37.314945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710b4798-88a4-4264-a7c0-f73a7e100294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:37.342414Z",
     "iopub.status.busy": "2024-01-25T01:40:37.342100Z",
     "iopub.status.idle": "2024-01-25T01:40:37.349238Z",
     "shell.execute_reply": "2024-01-25T01:40:37.347819Z",
     "shell.execute_reply.started": "2024-01-25T01:40:37.342382Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "model_check = ModelCheckpoint('convnet_for_GTSRB_with_Xception_ImageNet_weights.keras', monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "csv_logger = CSVLogger('train_log.csv', separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed94ae38-4a11-44c5-b625-ada773e7a847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T01:40:37.351351Z",
     "iopub.status.busy": "2024-01-25T01:40:37.350414Z",
     "iopub.status.idle": "2024-01-25T05:22:20.014565Z",
     "shell.execute_reply": "2024-01-25T05:22:20.013812Z",
     "shell.execute_reply.started": "2024-01-25T01:40:37.351293Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "981/981 [==============================] - 1731s 2s/step - loss: 0.2670 - accuracy: 0.9304 - val_loss: 0.0583 - val_accuracy: 0.9858 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "981/981 [==============================] - 1719s 2s/step - loss: 0.0420 - accuracy: 0.9903 - val_loss: 0.0655 - val_accuracy: 0.9865 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "981/981 [==============================] - 1126s 1s/step - loss: 0.0331 - accuracy: 0.9919 - val_loss: 0.0462 - val_accuracy: 0.9871 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "981/981 [==============================] - 1001s 1s/step - loss: 0.0264 - accuracy: 0.9944 - val_loss: 0.0175 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "981/981 [==============================] - 937s 955ms/step - loss: 0.0204 - accuracy: 0.9960 - val_loss: 0.0904 - val_accuracy: 0.9847 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "981/981 [==============================] - 976s 995ms/step - loss: 0.0198 - accuracy: 0.9957 - val_loss: 0.0782 - val_accuracy: 0.9861 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "981/981 [==============================] - 1157s 1s/step - loss: 0.0207 - accuracy: 0.9955 - val_loss: 0.0153 - val_accuracy: 0.9968 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "981/981 [==============================] - 867s 884ms/step - loss: 0.0195 - accuracy: 0.9956 - val_loss: 0.0926 - val_accuracy: 0.9837 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "981/981 [==============================] - 922s 940ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.2430 - val_accuracy: 0.9801 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "981/981 [==============================] - 1127s 1s/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.0988 - val_accuracy: 0.9888 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "981/981 [==============================] - 871s 888ms/step - loss: 0.0144 - accuracy: 0.9968 - val_loss: 0.0668 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "981/981 [==============================] - 869s 885ms/step - loss: 0.0142 - accuracy: 0.9975 - val_loss: 0.1068 - val_accuracy: 0.9874 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "history =  model.fit(x_train, y_train,  batch_size = 32, epochs = n_epochs, verbose = 1, \n",
    "              validation_data = (x_val, y_val), callbacks = [model_check, early, reduce_lr, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffeeb259-de15-4cdf-aba7-f64da2c5df17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T05:22:20.015880Z",
     "iopub.status.busy": "2024-01-25T05:22:20.015576Z",
     "iopub.status.idle": "2024-01-25T05:22:21.809711Z",
     "shell.execute_reply": "2024-01-25T05:22:21.808994Z",
     "shell.execute_reply.started": "2024-01-25T05:22:20.015856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('convnet_for_GTSRB_with_Xception_ImageNet_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d233f5b-40b9-4a0d-b44c-94c2925b0928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T05:36:17.548830Z",
     "iopub.status.busy": "2024-01-25T05:36:17.548446Z",
     "iopub.status.idle": "2024-01-25T05:38:11.525483Z",
     "shell.execute_reply": "2024-01-25T05:38:11.524738Z",
     "shell.execute_reply.started": "2024-01-25T05:36:17.548804Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 75s 188ms/step\n",
      "Accuracy on test dataset using CNN after transfer learning with Xception:  0.9812351543942993\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "test_model = keras.models.load_model(\"convnet_for_GTSRB_with_Xception_ImageNet_weights.keras\")\n",
    "predictions = test_model.predict(test_data)\n",
    "classes_x = np.argmax(predictions, axis = 1).tolist()\n",
    "classes_x = np.array([classes_x]).tolist()[0]\n",
    "\n",
    "print(\"Accuracy on test dataset using CNN after transfer learning with Xception: \", accuracy_score(test_labels, classes_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dde8a3-f68f-426a-b7c7-978e52010598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
