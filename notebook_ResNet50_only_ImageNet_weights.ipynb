{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392bb383-72f0-4082-802f-bc408795670e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:12.443044Z",
     "iopub.status.busy": "2024-01-24T23:16:12.442409Z",
     "iopub.status.idle": "2024-01-24T23:16:15.491169Z",
     "shell.execute_reply": "2024-01-24T23:16:15.490452Z",
     "shell.execute_reply.started": "2024-01-24T23:16:12.443007Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 23:16:13.724734: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-24 23:16:13.760471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38214686-0065-40e7-962c-5edadbda9731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:15.492903Z",
     "iopub.status.busy": "2024-01-24T23:16:15.492435Z",
     "iopub.status.idle": "2024-01-24T23:16:25.390351Z",
     "shell.execute_reply": "2024-01-24T23:16:25.389721Z",
     "shell.execute_reply.started": "2024-01-24T23:16:15.492878Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]:  [[[137  97  91]\n",
      "  [178 113 117]\n",
      "  [157  67  63]\n",
      "  ...\n",
      "  [176  60  41]\n",
      "  [186 111  66]\n",
      "  [247 230 225]]\n",
      "\n",
      " [[167 114 120]\n",
      "  [169  89 110]\n",
      "  [153  48  50]\n",
      "  ...\n",
      "  [155  68  46]\n",
      "  [178 112  57]\n",
      "  [248 215 188]]\n",
      "\n",
      " [[177 114 105]\n",
      "  [165  75  77]\n",
      "  [156  45  42]\n",
      "  ...\n",
      "  [180 121 102]\n",
      "  [185 125  95]\n",
      "  [248 186 183]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[140 166 198]\n",
      "  [116 134 166]\n",
      "  [106 119 101]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[148 191 211]\n",
      "  [116 149 165]\n",
      "  [157 183 167]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[100 155 146]\n",
      "  [ 73 102  86]\n",
      "  [103 112 100]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "labels[0: ] 0\n"
     ]
    }
   ],
   "source": [
    "imgs_path = \"./Train\"\n",
    "data = []\n",
    "labels = []\n",
    "CLASSES = 43\n",
    "# using for loop to access each image\n",
    "for i in range(CLASSES):\n",
    "    img_path = os.path.join(imgs_path, str(i)) #0-42\n",
    "    for img in os.listdir(img_path):\n",
    "        im = Image.open(imgs_path + '/' + str(i) + '/' + img)\n",
    "        im = im.resize((32,32))\n",
    "        im = np.array(im)\n",
    "        data.append(im)\n",
    "        labels.append(i)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(\"data[0]: \",data[0])\n",
    "print(\"labels[0: ]\",labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2fbe31-f672-47c8-8bd7-43ceb7c085db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:25.391733Z",
     "iopub.status.busy": "2024-01-24T23:16:25.391462Z",
     "iopub.status.idle": "2024-01-24T23:16:25.444313Z",
     "shell.execute_reply": "2024-01-24T23:16:25.442821Z",
     "shell.execute_reply.started": "2024-01-24T23:16:25.391708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape:  (31367, 32, 32, 3) (31367,)\n",
      "testing shape:  (7842, 32, 32, 3) (7842,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(\"training shape: \",x_train.shape, y_train.shape)\n",
    "print(\"testing shape: \",x_val.shape, y_val.shape)\n",
    "# convert interge label to one-hot data\n",
    "y_train = to_categorical(y_train, 43)\n",
    "y_val = to_categorical(y_val, 43)\n",
    "\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d3bdeb-3e16-4c66-b965-f4a2c6dabc16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T02:59:37.387262Z",
     "iopub.status.busy": "2024-01-25T02:59:37.386883Z",
     "iopub.status.idle": "2024-01-25T02:59:43.483115Z",
     "shell.execute_reply": "2024-01-25T02:59:43.481982Z",
     "shell.execute_reply.started": "2024-01-25T02:59:37.387233Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test = pd.read_csv(\"./Test.csv\")\n",
    "test_labels = test['ClassId'].values.tolist()\n",
    "\n",
    "test_img_path = \"./\"\n",
    "test_imgs = test['Path'].values\n",
    "test_data = []\n",
    "\n",
    "for img in test_imgs:\n",
    "    im = Image.open(test_img_path + '/' + img)\n",
    "    im = im.resize((32,32))\n",
    "    im = np.array(im)\n",
    "    test_data.append(im)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e04a5d7-68d8-45d2-b2ed-368f3eb8b5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:25.445867Z",
     "iopub.status.busy": "2024-01-24T23:16:25.445465Z",
     "iopub.status.idle": "2024-01-24T23:16:25.451463Z",
     "shell.execute_reply": "2024-01-24T23:16:25.450582Z",
     "shell.execute_reply.started": "2024-01-24T23:16:25.445833Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486960f-ef58-4b34-a82b-a1938c167f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f9ec9-714b-42fa-9462-cc979722153d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9da4649b-45ac-4500-a725-3af9dabe88ce",
   "metadata": {},
   "source": [
    "# Transfer Learning using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eed8abd-6a63-4687-b9ad-ca51ced45578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:25.454177Z",
     "iopub.status.busy": "2024-01-24T23:16:25.453870Z",
     "iopub.status.idle": "2024-01-24T23:16:27.261383Z",
     "shell.execute_reply": "2024-01-24T23:16:27.260666Z",
     "shell.execute_reply.started": "2024-01-24T23:16:25.454145Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "\n",
    "\n",
    "resnet = ResNet50(weights= 'imagenet', include_top=False, input_shape= (32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1fa215-371f-4f63-9b1a-9be241955e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:27.264423Z",
     "iopub.status.busy": "2024-01-24T23:16:27.264004Z",
     "iopub.status.idle": "2024-01-24T23:16:27.295686Z",
     "shell.execute_reply": "2024-01-24T23:16:27.295078Z",
     "shell.execute_reply.started": "2024-01-24T23:16:27.264387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(43, activation= 'softmax')(x)\n",
    "model = Model(inputs = resnet.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0dda9ef-22ea-4e36-9c51-335b441865ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:27.297048Z",
     "iopub.status.busy": "2024-01-24T23:16:27.296656Z",
     "iopub.status.idle": "2024-01-24T23:16:27.313950Z",
     "shell.execute_reply": "2024-01-24T23:16:27.313366Z",
     "shell.execute_reply.started": "2024-01-24T23:16:27.297014Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "710b4798-88a4-4264-a7c0-f73a7e100294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:27.315329Z",
     "iopub.status.busy": "2024-01-24T23:16:27.314941Z",
     "iopub.status.idle": "2024-01-24T23:16:27.321245Z",
     "shell.execute_reply": "2024-01-24T23:16:27.320503Z",
     "shell.execute_reply.started": "2024-01-24T23:16:27.315295Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "model_check = ModelCheckpoint('convnet_for_GTSRB_with_ResNet50_ImageNet_weights.keras', monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "csv_logger = CSVLogger('train_log.csv', separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed94ae38-4a11-44c5-b625-ada773e7a847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T23:16:27.322641Z",
     "iopub.status.busy": "2024-01-24T23:16:27.322214Z",
     "iopub.status.idle": "2024-01-25T02:46:51.651442Z",
     "shell.execute_reply": "2024-01-25T02:46:51.650306Z",
     "shell.execute_reply.started": "2024-01-24T23:16:27.322588Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "981/981 [==============================] - 603s 601ms/step - loss: 3.0987 - accuracy: 0.3310 - val_loss: 2.1936 - val_accuracy: 0.3577 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "981/981 [==============================] - 589s 601ms/step - loss: 1.9523 - accuracy: 0.5401 - val_loss: 55.7118 - val_accuracy: 0.2464 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "981/981 [==============================] - 590s 602ms/step - loss: 0.7296 - accuracy: 0.7808 - val_loss: 0.2890 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "981/981 [==============================] - 833s 850ms/step - loss: 0.3619 - accuracy: 0.9024 - val_loss: 0.2652 - val_accuracy: 0.9185 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "981/981 [==============================] - 876s 893ms/step - loss: 0.4631 - accuracy: 0.8850 - val_loss: 0.2652 - val_accuracy: 0.9211 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "981/981 [==============================] - 871s 888ms/step - loss: 0.2389 - accuracy: 0.9381 - val_loss: 0.8435 - val_accuracy: 0.8279 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "981/981 [==============================] - 867s 884ms/step - loss: 0.1817 - accuracy: 0.9530 - val_loss: 0.0756 - val_accuracy: 0.9783 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "981/981 [==============================] - 865s 882ms/step - loss: 0.1988 - accuracy: 0.9562 - val_loss: 0.1688 - val_accuracy: 0.9468 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "981/981 [==============================] - 640s 652ms/step - loss: 0.1366 - accuracy: 0.9669 - val_loss: 0.0604 - val_accuracy: 0.9810 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "981/981 [==============================] - 591s 602ms/step - loss: 0.0782 - accuracy: 0.9801 - val_loss: 0.0576 - val_accuracy: 0.9846 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "981/981 [==============================] - 325s 331ms/step - loss: 0.1069 - accuracy: 0.9748 - val_loss: 0.0901 - val_accuracy: 0.9750 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "981/981 [==============================] - 486s 495ms/step - loss: 0.1115 - accuracy: 0.9737 - val_loss: 0.0504 - val_accuracy: 0.9848 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "981/981 [==============================] - 524s 534ms/step - loss: 0.0625 - accuracy: 0.9837 - val_loss: 0.0430 - val_accuracy: 0.9874 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "981/981 [==============================] - 569s 580ms/step - loss: 0.0629 - accuracy: 0.9847 - val_loss: 0.0636 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "981/981 [==============================] - 577s 588ms/step - loss: 0.0669 - accuracy: 0.9835 - val_loss: 0.0344 - val_accuracy: 0.9916 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "981/981 [==============================] - 556s 567ms/step - loss: 0.0810 - accuracy: 0.9829 - val_loss: 0.1590 - val_accuracy: 0.9560 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "981/981 [==============================] - 567s 578ms/step - loss: 0.0548 - accuracy: 0.9867 - val_loss: 0.0438 - val_accuracy: 0.9876 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "981/981 [==============================] - 570s 581ms/step - loss: 0.0470 - accuracy: 0.9885 - val_loss: 0.0489 - val_accuracy: 0.9869 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "981/981 [==============================] - 560s 571ms/step - loss: 0.0301 - accuracy: 0.9924 - val_loss: 0.0537 - val_accuracy: 0.9869 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "981/981 [==============================] - 564s 575ms/step - loss: 0.0510 - accuracy: 0.9880 - val_loss: 0.0842 - val_accuracy: 0.9807 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "history =  model.fit(x_train, y_train,  batch_size = 32, epochs = n_epochs, verbose = 1, \n",
    "              validation_data = (x_val, y_val), callbacks = [model_check, early, reduce_lr, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffeeb259-de15-4cdf-aba7-f64da2c5df17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T02:46:51.657055Z",
     "iopub.status.busy": "2024-01-25T02:46:51.656667Z",
     "iopub.status.idle": "2024-01-25T02:47:06.472679Z",
     "shell.execute_reply": "2024-01-25T02:47:06.471306Z",
     "shell.execute_reply.started": "2024-01-25T02:46:51.657012Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('convnet_for_GTSRB_with_ResNet50_ImageNet_weights.keras')\n",
    "# model.save('convnet_for_GTSRB_with_ResNet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d233f5b-40b9-4a0d-b44c-94c2925b0928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T03:02:00.463239Z",
     "iopub.status.busy": "2024-01-25T03:02:00.462286Z",
     "iopub.status.idle": "2024-01-25T03:06:50.001607Z",
     "shell.execute_reply": "2024-01-25T03:06:49.987590Z",
     "shell.execute_reply.started": "2024-01-25T03:02:00.463181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 69s 170ms/step\n",
      "Accuracy on test dataset using CNN after transfer learning with ResNet50 using ImageNet weights:  0.94972288202692\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "test_model = keras.models.load_model(\"convnet_for_GTSRB_with_ResNet50_ImageNet_weights.keras\")\n",
    "predictions = test_model.predict(test_data)\n",
    "classes_x = np.argmax(predictions, axis = 1).tolist()\n",
    "classes_x = np.array([classes_x]).tolist()[0]\n",
    "\n",
    "print(\"Accuracy on test dataset using CNN after transfer learning with ResNet50 using ImageNet weights: \", accuracy_score(test_labels, classes_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dde8a3-f68f-426a-b7c7-978e52010598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
